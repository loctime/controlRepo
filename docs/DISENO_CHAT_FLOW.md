âš ï¸ Este servicio NO es pÃºblico.
âš ï¸ Este servicio es consumido exclusivamente por ControlFile.
âš ï¸ El frontend nunca debe llamar a este servicio.
# ğŸ§  DiseÃ±o del Flujo de Chat - ControlFile + ControlRepo

**Estado:** ğŸ“‹ DiseÃ±o Propuesto (NO implementado)  
**Fecha:** 2024-12-XX  
**VersiÃ³n:** 1.0.0

---

## ğŸ¯ Objetivo

DiseÃ±ar y documentar el flujo definitivo de chat usando:
- **ControlFile** como orquestador Ãºnico
- **ControlRepo** como servicio de inteligencia (LLM Service)

---

## âœ… DecisiÃ³n ArquitectÃ³nica Adoptada

**OpciÃ³n A: ControlFile Orquesta, ControlRepo Ejecuta**

- **ControlFile**: Orquestador central, endpoint pÃºblico, validaciÃ³n, normalizaciÃ³n
- **ControlRepo**: Servicio interno de LLM, RAG, soporte de motores (Ollama local, cloud futuro)

---

## ğŸ”„ Flujo Completo Paso a Paso

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend   â”‚
â”‚  (Next.js)  â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ POST /api/chat/query
       â”‚ {
       â”‚   repositoryId: "github:owner:repo",
       â”‚   question: "Â¿CÃ³mo funciona X?",
       â”‚   conversationId: "conv-123" (opcional),
       â”‚   role: "developer" (opcional)
       â”‚ }
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ControlFile Backend                      â”‚
â”‚                    (Express.js)                              â”‚
â”‚                                                              â”‚
â”‚  1. Validar autenticaciÃ³n (Firebase Auth)                    â”‚
â”‚  2. Validar repositoryId (formato, existencia)               â”‚
â”‚  3. Verificar estado del repositorio                         â”‚
â”‚     - Si 'indexing' â†’ 202 Accepted                           â”‚
â”‚     - Si 'idle' o 'error' â†’ 400 Bad Request                  â”‚
â”‚     - Si 'ready' â†’ Continuar                                 â”‚
â”‚  4. Cargar Ã­ndice del repositorio (filesystem)               â”‚
â”‚  5. Preparar contexto (Project Brain, mÃ©tricas si existen)  â”‚
â”‚  6. Orquestar llamada a ControlRepo                          â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ POST /internal/llm/query
       â”‚ {
       â”‚   question: "...",
       â”‚   repositoryId: "...",
       â”‚   role: "developer",
       â”‚   conversationMemory: [...],
       â”‚   context: {
       â”‚     index: {...},           // Ãndice completo
       â”‚     projectBrain: {...},    // Si existe
       â”‚     metrics: {...}          // Si existe
       â”‚   },
       â”‚   options: {
       â”‚     engine: "ollama" | "cloud",
       â”‚     model: "phi3:mini",
       â”‚     temperature: 0.7
       â”‚   }
       â”‚ }
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ControlRepo Service                       â”‚
â”‚                    (LLM Service)                             â”‚
â”‚                                                              â”‚
â”‚  1. Recibir query y contexto                                 â”‚
â”‚  2. Ejecutar RAG (Retrieval Augmented Generation)            â”‚
â”‚     - Buscar archivos relevantes en el Ã­ndice                â”‚
â”‚     - Extraer contexto especÃ­fico                            â”‚
â”‚     - Combinar con Project Brain y mÃ©tricas                   â”‚
â”‚  3. Construir prompt completo                                â”‚
â”‚  4. Llamar al motor LLM seleccionado:                        â”‚
â”‚     - Dev: Ollama local (phi3:mini)                          â”‚
â”‚     - Prod: Cloud LLM (futuro)                               â”‚
â”‚  5. Generar respuesta con citas a fuentes                   â”‚
â”‚  6. Retornar respuesta estructurada                         â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 200 OK
       â”‚ {
       â”‚   answer: "La respuesta generada...",
       â”‚   files: [
       â”‚     {
       â”‚       path: "src/auth.ts",
       â”‚       lines: [10, 25],
       â”‚       relevance: 0.95
       â”‚     }
       â”‚   ],
       â”‚   findings: [
       â”‚     {
       â”‚       type: "function",
       â”‚       name: "authenticate",
       â”‚       path: "src/auth.ts",
       â”‚       line: 15
       â”‚     }
       â”‚   ],
       â”‚   debug: {
       â”‚     engine: "ollama",
       â”‚     model: "phi3:mini",
       â”‚     location: "local",
       â”‚     tokensUsed: 150,
       â”‚     latency: 1.2
       â”‚   }
       â”‚ }
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ControlFile Backend                      â”‚
â”‚                                                              â”‚
â”‚  7. Normalizar respuesta                                      â”‚
â”‚     - Mapear formato interno â†’ formato pÃºblico               â”‚
â”‚     - Validar estructura                                     â”‚
â”‚     - Agregar metadata de ControlFile                        â”‚
â”‚  8. Manejar fallback si ControlRepo falla                    â”‚
â”‚  9. Retornar respuesta al frontend                           â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”‚ 200 OK
       â”‚ {
       â”‚   response: "La respuesta generada...",
       â”‚   conversationId: "conv-123",
       â”‚   sources: [
       â”‚     {
       â”‚       path: "src/auth.ts",
       â”‚       lines: [10, 25]
       â”‚     }
       â”‚   ],
       â”‚   debug: {              // Solo en modo dev
       â”‚     engine: "ollama",
       â”‚     model: "phi3:mini",
       â”‚     location: "local"
       â”‚   }
       â”‚ }
       â”‚
       â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Frontend   â”‚
â”‚  (Next.js)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Responsabilidades por Sistema

### ControlFile (Orquestador)

#### âœ… Responsabilidades

1. **Endpoint PÃºblico de Chat**
   - `POST /api/chat/query` - Ãšnico endpoint expuesto al frontend
   - ValidaciÃ³n de autenticaciÃ³n (Firebase Auth)
   - Rate limiting y seguridad

2. **ValidaciÃ³n de Usuario/Repositorio**
   - Validar formato de `repositoryId` (`github:owner:repo`)
   - Verificar que el usuario tiene acceso al repositorio
   - Verificar estado del repositorio (`ready`, `indexing`, `idle`, `error`)

3. **OrquestaciÃ³n**
   - Cargar Ã­ndice completo desde filesystem
   - Cargar Project Brain y mÃ©tricas (si existen)
   - Preparar contexto para ControlRepo
   - Llamar a ControlRepo con payload estructurado
   - Manejar timeouts y errores de ControlRepo

4. **NormalizaciÃ³n de Respuestas**
   - Mapear respuesta interna de ControlRepo â†’ formato pÃºblico
   - Validar estructura de respuesta
   - Agregar metadata de ControlFile (timestamps, etc.)
   - Filtrar informaciÃ³n sensible (debug solo en dev)

5. **Manejo de Fallback**
   - Si ControlRepo no responde â†’ respuesta degradada
   - Si ControlRepo falla â†’ error controlado con mensaje claro
   - Logging detallado para debugging

#### âŒ NO Responsabilidades

- âŒ NO ejecuta LLM directamente
- âŒ NO hace RAG directamente
- âŒ NO gestiona motores LLM (Ollama, cloud)
- âŒ NO genera embeddings
- âŒ NO procesa prompts complejos

---

### ControlRepo (Servicio de Inteligencia)

#### âœ… Responsabilidades

1. **Servicio Interno de LLM**
   - `POST /internal/llm/query` - Endpoint interno (NO pÃºblico)
   - Procesar queries con contexto completo
   - Ejecutar RAG sobre el Ã­ndice proporcionado

2. **RAG (Retrieval Augmented Generation)**
   - Buscar archivos relevantes en el Ã­ndice
   - Extraer contexto especÃ­fico segÃºn la pregunta
   - Combinar con Project Brain y mÃ©tricas
   - Generar embeddings si es necesario (futuro)

3. **Soporte de Motores LLM**
   - **Modo Dev**: Ollama local (`phi3:mini`)
   - **Modo Prod**: Cloud LLM (OpenAI, Anthropic, etc. - futuro)
   - SelecciÃ³n automÃ¡tica segÃºn configuraciÃ³n
   - Fallback entre motores si uno falla

4. **GeneraciÃ³n de Respuestas**
   - Construir prompt completo con contexto
   - Llamar al motor LLM seleccionado
   - Generar respuesta con citas a fuentes
   - Extraer findings (funciones, clases, etc.)

5. **Debug y Visibilidad**
   - Retornar informaciÃ³n del motor usado
   - Retornar mÃ©tricas (tokens, latency)
   - Retornar ubicaciÃ³n (local vs cloud)

#### âŒ NO Responsabilidades

- âŒ NO valida autenticaciÃ³n de usuarios
- âŒ NO valida acceso a repositorios
- âŒ NO gestiona Ã­ndices (solo los consume)
- âŒ NO expone endpoints pÃºblicos
- âŒ NO normaliza respuestas para frontend

---

## ğŸ”Œ Contrato HTTP: ControlFile â†’ ControlRepo

### Endpoint

```
POST /internal/llm/query
```

**âš ï¸ IMPORTANTE:** Este endpoint es **INTERNO** y **NO debe exponerse pÃºblicamente**.

- Solo debe ser accesible desde ControlFile
- ValidaciÃ³n mediante header `X-ControlFile-Signature` (futuro)
- O mediante red privada/VPN (producciÃ³n)

**ğŸ“ Nota Futura - Naming del Endpoint:**
- `/internal/llm/query` estÃ¡ bien para la implementaciÃ³n inicial
- A largo plazo, cuando el servicio crezca, podrÃ­as necesitar:
  - `/internal/ai/chat` - Para queries de chat
  - `/internal/ai/flows` - Para flujos complejos de anÃ¡lisis
  - `/internal/ai/analysis` - Para anÃ¡lisis estÃ¡ticos
- No es urgente, pero tenlo en mente para la evoluciÃ³n del servicio

---

### Request

```typescript
interface LLMQueryRequest {
  // Datos de la consulta
  question: string;                    // REQUERIDO: Pregunta del usuario
  repositoryId: string;                 // REQUERIDO: ID del repositorio (github:owner:repo)
  
  // Contexto de conversaciÃ³n
  conversationMemory?: Array<{        // OPCIONAL: Historial de conversaciÃ³n
    role: "user" | "assistant";
    content: string;
    timestamp: string;
  }>;
  
  // Rol del usuario (afecta el tipo de respuesta)
  role?: "developer" | "manager" | "analyst";  // OPCIONAL: Default "developer"
  
  // Contexto completo del repositorio
  context: {
    mode?: "full" | "references";      // OPCIONAL: Modo de contexto (ver nota futura)
    index: {                           // REQUERIDO: Ãndice completo del repositorio
      files: Array<{
        path: string;
        content: string;               // âš ï¸ FUTURO: En modo "references", solo IDs/paths
        language?: string;
        size: number;
      }>;
      tree: any;                       // Estructura de directorios
      stats: {
        totalFiles: number;
        totalSize: number;
        languages: Record<string, number>;
      };
    };
    projectBrain?: {                   // OPCIONAL: Project Brain si existe
      summary: string;
      architecture: string;
      keyComponents: Array<string>;
      dependencies: Array<string>;
    };
    metrics?: {                        // OPCIONAL: MÃ©tricas si existen
      complexity: number;
      testCoverage?: number;
      documentation?: number;
    };
  };
  
  // Opciones de ejecuciÃ³n
  options?: {
    engine?: "ollama" | "cloud";       // OPCIONAL: Preferencia de motor
    model?: string;                    // OPCIONAL: Modelo especÃ­fico (ej: "phi3:mini")
    temperature?: number;              // OPCIONAL: Default 0.7
    maxTokens?: number;                // OPCIONAL: LÃ­mite de tokens
    includeDebug?: boolean;            // OPCIONAL: Incluir info de debug
  };
}
```

**Ejemplo:**

```json
{
  "question": "Â¿CÃ³mo funciona la autenticaciÃ³n en este proyecto?",
  "repositoryId": "github:owner:repo",
  "role": "developer",
  "conversationMemory": [
    {
      "role": "user",
      "content": "Â¿QuÃ© tecnologÃ­as usa este proyecto?",
      "timestamp": "2024-01-01T12:00:00Z"
    },
    {
      "role": "assistant",
      "content": "Este proyecto usa TypeScript, React y Firebase...",
      "timestamp": "2024-01-01T12:00:05Z"
    }
  ],
  "context": {
    "index": {
      "files": [...],
      "tree": {...},
      "stats": {...}
    },
    "projectBrain": {
      "summary": "AplicaciÃ³n web moderna...",
      "architecture": "MVC con componentes React...",
      "keyComponents": ["AuthService", "FileManager"],
      "dependencies": ["react", "firebase"]
    },
    "metrics": {
      "complexity": 7.5,
      "testCoverage": 65
    }
  },
  "options": {
    "engine": "ollama",
    "model": "phi3:mini",
    "temperature": 0.7,
    "includeDebug": true
  }
}
```

**ğŸ“ Nota Futura - OptimizaciÃ³n de Contexto Pesado:**

âš ï¸ **ConsideraciÃ³n importante:** Para repositorios grandes, enviar el contenido completo de todos los archivos (`context.index.files[].content`) puede ser muy pesado y ControlRepo no siempre necesita TODO el contenido.

**OptimizaciÃ³n futura propuesta:**

1. **Modo "references"** (producciÃ³n):
   ```typescript
   context: {
     mode: "references",
     index: {
       files: Array<{
         path: string;
         // content: NO se envÃ­a
         fileId: string;              // ID para solicitar fragmentos despuÃ©s
         language?: string;
         size: number;
         relevance?: number;          // Score inicial de relevancia
       }>
     }
   }
   ```
   - ControlRepo primero identifica archivos relevantes
   - Luego solicita fragmentos especÃ­ficos: `POST /internal/ai/fragments` con `{ fileIds: [...], ranges: [...] }`
   - ControlFile responde solo con los fragmentos solicitados

2. **Modo "full"** (desarrollo):
   ```typescript
   context: {
     mode: "full",
     index: {
       files: Array<{
         path: string;
         content: string;             // Contenido completo
         // ...
       }>
     }
   }
   ```
   - Ãštil para desarrollo y repos pequeÃ±os
   - Evita round-trips adicionales

**ImplementaciÃ³n futura:**
- No cambiar ahora, solo tenerlo en mente
- Evaluar cuando repos grandes (>1000 archivos) empiecen a causar problemas de performance
- Considerar lÃ­mite de tamaÃ±o antes de activar modo "references" automÃ¡ticamente

---

### Response

```typescript
interface LLMQueryResponse {
  // Respuesta principal
  answer: string;                     // REQUERIDO: Respuesta generada por el LLM
  
  // Archivos citados en la respuesta
  files: Array<{                      // REQUERIDO: Archivos relevantes encontrados
    path: string;
    lines: [number, number];          // Rango de lÃ­neas relevantes [start, end]
    relevance: number;                // Score de relevancia (0-1)
    excerpt?: string;                 // OPCIONAL: Fragmento del cÃ³digo citado
  }>;
  
  // Hallazgos estructurados
  findings?: Array<{                  // OPCIONAL: Entidades encontradas
    type: "function" | "class" | "interface" | "variable" | "import";
    name: string;
    path: string;
    line: number;
    description?: string;
  }>;
  
  // InformaciÃ³n de debug (solo si includeDebug: true)
  debug?: {
    engine: "ollama" | "cloud";
    model: string;
    location: "local" | "cloud";
    tokensUsed: number;
    latency: number;                  // Segundos
    retrievalTime?: number;           // Tiempo de RAG
    generationTime?: number;          // Tiempo de generaciÃ³n
  };
  
  // Metadata
  timestamp: string;                  // ISO 8601
  conversationId?: string;            // OPCIONAL: ID de conversaciÃ³n si se proporcionÃ³
}
```

**Ejemplo:**

```json
{
  "answer": "La autenticaciÃ³n en este proyecto funciona mediante Firebase Auth. El componente principal es `AuthService` ubicado en `src/services/auth.ts`. Utiliza tokens JWT y maneja sesiones mediante cookies seguras...",
  "files": [
    {
      "path": "src/services/auth.ts",
      "lines": [10, 45],
      "relevance": 0.95,
      "excerpt": "export class AuthService {\n  async authenticate(token: string) {\n    // ...\n  }\n}"
    },
    {
      "path": "src/middleware/auth.ts",
      "lines": [5, 20],
      "relevance": 0.82
    }
  ],
  "findings": [
    {
      "type": "class",
      "name": "AuthService",
      "path": "src/services/auth.ts",
      "line": 15,
      "description": "Servicio principal de autenticaciÃ³n"
    },
    {
      "type": "function",
      "name": "authenticate",
      "path": "src/services/auth.ts",
      "line": 20
    }
  ],
  "debug": {
    "engine": "ollama",
    "model": "phi3:mini",
    "location": "local",
    "tokensUsed": 150,
    "latency": 1.2,
    "retrievalTime": 0.3,
    "generationTime": 0.9
  },
  "timestamp": "2024-01-01T12:00:10Z",
  "conversationId": "conv-123"
}
```

---

### CÃ³digos de Estado HTTP

- **200 OK**: Query procesada exitosamente
- **400 Bad Request**: Request invÃ¡lido (falta `question`, `repositoryId`, etc.)
- **500 Internal Server Error**: Error en ControlRepo (LLM fallÃ³, RAG fallÃ³, etc.)
- **503 Service Unavailable**: ControlRepo no disponible (servicio caÃ­do)

---

## ğŸ›¡ï¸ Manejo de Fallback

### Escenario 1: ControlRepo No Responde (Timeout)

**Comportamiento:**
1. ControlFile espera mÃ¡ximo 30 segundos
2. Si timeout â†’ Retornar respuesta degradada

**Respuesta Degradada:**
```json
{
  "response": "Lo siento, el servicio de inteligencia no estÃ¡ disponible en este momento. Por favor, intenta de nuevo mÃ¡s tarde.",
  "conversationId": "conv-123",
  "sources": [],
  "error": {
    "type": "service_unavailable",
    "message": "ControlRepo no respondiÃ³ en el tiempo esperado"
  }
}
```

**CÃ³digo HTTP:** `503 Service Unavailable`

**ğŸ“ Nota Futura - Fallback Degradado Mejorado:**
- El fallback actual devuelve un mensaje genÃ©rico
- A futuro, podrÃ­as mejorar la UX con:
  - **Search-only mode**: Devolver resultados de bÃºsqueda sin LLM (usando lÃ³gica de bÃºsqueda simple)
  - **Reutilizar lÃ³gica legacy**: Usar la implementaciÃ³n bÃ¡sica de `processQuery` que existe en `chat-service.js`
  - Esto proporcionarÃ­a valor incluso cuando ControlRepo no estÃ¡ disponible
- Es una mejora UX, no un requisito para la implementaciÃ³n inicial

---

### Escenario 2: ControlRepo Retorna Error 500

**Comportamiento:**
1. ControlFile recibe error de ControlRepo
2. Loggear error detallado
3. Retornar error controlado al frontend

**Respuesta:**
```json
{
  "error": "Error procesando query",
  "message": "El servicio de inteligencia encontrÃ³ un error. Por favor, intenta reformular tu pregunta.",
  "conversationId": "conv-123"
}
```

**CÃ³digo HTTP:** `500 Internal Server Error`

---

### Escenario 3: ControlRepo No Disponible (Servicio CaÃ­do)

**Comportamiento:**
1. ControlFile detecta que ControlRepo no estÃ¡ disponible
2. Health check falla
3. Retornar respuesta degradada inmediatamente (sin intentar llamar)

**Respuesta:**
```json
{
  "response": "El servicio de inteligencia no estÃ¡ disponible en este momento. Por favor, intenta mÃ¡s tarde.",
  "conversationId": "conv-123",
  "sources": [],
  "error": {
    "type": "service_unavailable",
    "message": "ControlRepo no estÃ¡ disponible"
  }
}
```

**CÃ³digo HTTP:** `503 Service Unavailable`

---

### Escenario 4: Modo Dev - Ollama No Disponible

**Comportamiento:**
1. ControlRepo detecta que Ollama no estÃ¡ corriendo
2. Retornar error especÃ­fico con instrucciones

**Respuesta desde ControlRepo:**
```json
{
  "error": "ollama_not_available",
  "message": "Ollama no estÃ¡ corriendo. Inicia Ollama con: ollama serve",
  "debug": {
    "engine": "ollama",
    "location": "local",
    "available": false
  }
}
```

**ControlFile normaliza a:**
```json
{
  "error": "Error de configuraciÃ³n",
  "message": "El servicio de inteligencia local no estÃ¡ disponible. Verifica que Ollama estÃ© corriendo.",
  "conversationId": "conv-123"
}
```

---

## ğŸ” Debug y Visibilidad de Motor LLM

### InformaciÃ³n de Debug

La informaciÃ³n de debug se incluye en la respuesta **SOLO** cuando:
- `options.includeDebug === true` en el request
- O cuando `NODE_ENV === 'development'` en ControlFile

### Campos de Debug

```typescript
{
  debug: {
    engine: "ollama" | "cloud";      // Motor usado
    model: string;                    // Modelo especÃ­fico (ej: "phi3:mini")
    location: "local" | "cloud";      // DÃ³nde se ejecutÃ³
    tokensUsed: number;                // Tokens consumidos
    latency: number;                  // Latencia total (segundos)
    retrievalTime?: number;           // Tiempo de RAG
    generationTime?: number;          // Tiempo de generaciÃ³n LLM
  }
}
```

### Visibilidad en Frontend

- **Modo Dev**: Debug visible en respuesta
- **Modo Prod**: Debug filtrado (no se envÃ­a al frontend)

---

## ğŸ—ï¸ Modo Dev vs Modo Prod

### Modo Dev (Desarrollo Local)

**ControlRepo:**
- Motor LLM: Ollama local
- Modelo: `phi3:mini` (default)
- Endpoint: `http://localhost:PORT/internal/llm/query`
- Debug: Habilitado por defecto

**ControlFile:**
- `NODE_ENV=development`
- Debug incluido en respuestas
- Logs detallados
- Timeout mÃ¡s largo (60 segundos)

---

### Modo Prod (ProducciÃ³n)

**ControlRepo:**
- Motor LLM: Cloud LLM (futuro: OpenAI, Anthropic, etc.)
- Modelo: Configurable por entorno
- Endpoint: `https://controlrepo.controldoc.app/internal/llm/query`
- Debug: Solo si se solicita explÃ­citamente

**ControlFile:**
- `NODE_ENV=production`
- Debug filtrado de respuestas
- Logs estructurados
- Timeout estÃ¡ndar (30 segundos)
- Rate limiting estricto

---

## ğŸ“ Endpoints: Estado y MigraciÃ³n

### âœ… Endpoints que se MANTIENEN

1. **`POST /api/chat/query`** (ControlFile)
   - **Estado**: âœ… Se mantiene (endpoint pÃºblico principal)
   - **Cambios**: Internamente ahora llama a ControlRepo
   - **Frontend**: Sin cambios necesarios

---

### ğŸ”„ Endpoints que se RENOMBRAN o ENCAPSULAN

1. **`POST /internal/llm/query`** (ControlRepo) - **NUEVO**
   - **Estado**: ğŸ†• Nuevo endpoint interno
   - **Acceso**: Solo desde ControlFile (no pÃºblico)
   - **PropÃ³sito**: Reemplaza lÃ³gica LLM que estaba en ControlFile

---

### âŒ Endpoints que quedan OBSOLETOS

**Ninguno identificado actualmente.**

Si existen endpoints de chat en ControlRepo que el frontend llama directamente, estos deben:
1. Marcarse como deprecated
2. Redirigirse a travÃ©s de ControlFile
3. Eliminarse en versiÃ³n futura

---

## âœ… ConfirmaciÃ³n ExplÃ­cita

### ğŸš« El Frontend NUNCA Llama Directo al LLM

**GARANTÃA ARQUITECTÃ“NICA:**

```
Frontend â†’ ControlFile â†’ ControlRepo â†’ LLM
   âœ…          âœ…            âœ…         âœ…

Frontend â†’ ControlRepo â†’ LLM
   âŒ          âŒ            âŒ
```

**Razones:**
1. **Seguridad**: ControlFile valida autenticaciÃ³n y permisos
2. **OrquestaciÃ³n**: ControlFile carga Ã­ndices y contexto
3. **NormalizaciÃ³n**: ControlFile unifica formato de respuestas
4. **Fallback**: ControlFile maneja errores y degradaciÃ³n
5. **Observabilidad**: ControlFile centraliza logs y mÃ©tricas

**ImplementaciÃ³n:**
- El endpoint `/internal/llm/query` de ControlRepo **NO debe exponerse pÃºblicamente**
- Solo debe ser accesible desde ControlFile (red privada o validaciÃ³n de origen)
- El frontend **SOLO** conoce `/api/chat/query` en ControlFile

---

## ğŸ“Š Diagrama de Responsabilidades

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Frontend                             â”‚
â”‚                                                             â”‚
â”‚  Responsabilidades:                                         â”‚
â”‚  - UI de chat                                               â”‚
â”‚  - Manejo de conversaciones                                 â”‚
â”‚  - Renderizado de respuestas y fuentes                      â”‚
â”‚                                                             â”‚
â”‚  Endpoints que usa:                                         â”‚
â”‚  âœ… POST /api/chat/query (ControlFile)                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ HTTP
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ControlFile Backend                     â”‚
â”‚                                                             â”‚
â”‚  Responsabilidades:                                         â”‚
â”‚  âœ… Endpoint pÃºblico /api/chat/query                        â”‚
â”‚  âœ… ValidaciÃ³n de autenticaciÃ³n                             â”‚
â”‚  âœ… ValidaciÃ³n de repositorio                                â”‚
â”‚  âœ… Carga de Ã­ndices (filesystem)                           â”‚
â”‚  âœ… Carga de Project Brain y mÃ©tricas                       â”‚
â”‚  âœ… OrquestaciÃ³n de llamada a ControlRepo                   â”‚
â”‚  âœ… NormalizaciÃ³n de respuestas                             â”‚
â”‚  âœ… Manejo de fallback                                      â”‚
â”‚                                                             â”‚
â”‚  Endpoints que expone:                                      â”‚
â”‚  âœ… POST /api/chat/query (pÃºblico)                          â”‚
â”‚                                                             â”‚
â”‚  Endpoints que llama:                                       â”‚
â”‚  âœ… POST /internal/llm/query (ControlRepo)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ HTTP Interno
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ControlRepo Service                      â”‚
â”‚                                                             â”‚
â”‚  Responsabilidades:                                         â”‚
â”‚  âœ… Servicio interno de LLM                                 â”‚
â”‚  âœ… RAG (Retrieval Augmented Generation)                    â”‚
â”‚  âœ… Soporte de motores (Ollama, Cloud)                      â”‚
â”‚  âœ… GeneraciÃ³n de respuestas                                â”‚
â”‚  âœ… ExtracciÃ³n de findings                                  â”‚
â”‚  âœ… Debug y mÃ©tricas                                        â”‚
â”‚                                                             â”‚
â”‚  Endpoints que expone:                                      â”‚
â”‚  âœ… POST /internal/llm/query (interno, NO pÃºblico)         â”‚
â”‚                                                             â”‚
â”‚  Motores LLM:                                               â”‚
â”‚  âœ… Dev: Ollama local (phi3:mini)                           â”‚
â”‚  âœ… Prod: Cloud LLM (futuro)                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â”‚ API Calls
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Motores LLM                              â”‚
â”‚                                                             â”‚
â”‚  Ollama (Dev):                                              â”‚
â”‚  - Modelo: phi3:mini                                        â”‚
â”‚  - UbicaciÃ³n: Local                                         â”‚
â”‚                                                             â”‚
â”‚  Cloud LLM (Prod - Futuro):                                 â”‚
â”‚  - OpenAI / Anthropic / etc.                                â”‚
â”‚  - UbicaciÃ³n: Cloud                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ” Seguridad

### ValidaciÃ³n de Origen (Futuro)

**ControlRepo debe validar que las requests vienen de ControlFile:**

1. **Header de Firma** (Recomendado):
   ```
   X-ControlFile-Signature: <HMAC_SHA256>
   ```

2. **Red Privada** (Alternativa):
   - ControlFile y ControlRepo en misma VPC
   - Firewall bloquea acceso externo a `/internal/llm/query`

3. **Token de Servicio** (Alternativa):
   ```
   Authorization: Bearer <SERVICE_TOKEN>
   ```

---

## ğŸ“ Notas de ImplementaciÃ³n Futura

### Fase 1: IntegraciÃ³n BÃ¡sica
- [ ] Implementar endpoint `/internal/llm/query` en ControlRepo
- [ ] Modificar `chat-service.js` en ControlFile para llamar a ControlRepo
- [ ] Implementar manejo de fallback bÃ¡sico
- [ ] Testing con Ollama local

### Fase 2: RAG Completo
- [ ] Implementar RAG en ControlRepo
- [ ] IntegraciÃ³n con Project Brain
- [ ] ExtracciÃ³n de findings estructurados
- [ ] OptimizaciÃ³n de bÃºsqueda de archivos relevantes

### Fase 3: ProducciÃ³n
- [ ] IntegraciÃ³n con Cloud LLM
- [ ] ValidaciÃ³n de origen (firma/red privada)
- [ ] MÃ©tricas y observabilidad
- [ ] Rate limiting en ControlRepo

---

## ğŸ“š Referencias

- [Arquitectura de Repositorios](./ARQUITECTURA_REPOSITORIOS.md)
- [Truth Document](./TRUTH.md)
- ControlRepo: ImplementaciÃ³n de LLM con Ollama (phi3:mini)

---

**Ãšltima actualizaciÃ³n:** 2024-12-XX  
**VersiÃ³n:** 1.0.0  
**Estado:** ğŸ“‹ DiseÃ±o Propuesto (NO implementado)
